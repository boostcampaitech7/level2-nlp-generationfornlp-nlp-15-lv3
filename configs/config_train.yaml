model:
  name: "beomi/gemma-ko-7b"  # 사용할 모델 이름
  trust_remote_code: true                                  # 원격 코드 신뢰 여부
  quantization:                                            # 4비트 양자화 관련 설정 추가
    enable: true                                           # 양자화 활성화
    load_in_4bit: true                                     # 4비트로 모델 로드
    bnb_4bit_compute_dtype: "float16"                      # 연산에 사용할 데이터 타입
    bnb_4bit_use_double_quant: true                        # 더블 양자화 활성화 여부
    bnb_4bit_quant_type: "nf4"                             # 양자화 유형 (nf4 또는 fp4 중 선택)

peft:
  enable: true                       # PEFT 활성화 여부
  r: 16                              # LoRA의 랭크
  lora_alpha: 16                     # LoRA 알파 값
  lora_dropout: 0.05                 # LoRA 드롭아웃 비율
  target_modules: ["q_proj", "k_proj"] # PEFT가 적용될 타겟 모듈
  task_type: "CAUSAL_LM"             # PEFT 작업 유형

training:
  output_dir: "outputs_gemma"        # 출력 디렉토리 경로
  batch_size:
    train: 1                         # 학습 배치 크기
    eval: 1                          # 평가 배치 크기
  epochs: 3                          # 학습 에폭 수
  learning_rate: 5.0e-6              # 학습률
  weight_decay: 0.01                 # Weight Decay 값
  logging_steps: 500                 # 로깅 간격 (스텝 기준)
  save_strategy: "epoch"             # 체크포인트 저장
  eval_strategy: "epoch"             # 평가
  save_total_limit: 2                # 저장할 체크포인트의 최대 개수
  fp16: true                         # Mixed Precision 사용 여부

data:
  dataset_path: "/data/ephemeral/level2-nlp-generationfornlp-nlp-15-lv3/data/train.csv"  # 데이터셋 경로
  max_seq_length: 2048                                # 입력 데이터의 최대 시퀀스 길이
  test_split_ratio: 0.1                               # 테스트 데이터 비율

seed: 42                                              # 난수 시드

